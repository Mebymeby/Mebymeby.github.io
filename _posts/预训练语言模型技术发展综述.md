# 预训练语言模型技术发展综述

> NLP 领域的主要进展表格

| 年份    | 主要技术                               |
| ------- | -------------------------------------- |
| 2013 年 | Word2Vec                               |
| 2014 年 | GloVe                                  |
| 2015 年 | LSTM、Attention                        |
| 2016 年 | Self-Attention                         |
| 2017 年 | Transformer                            |
| 2018 年 | ELMo、GPT、GNN                         |
| 2019 年 | BERT、XLNet、BoBERTa、GPT-2、ERNIE、T5 |
| 2020 年 | GPT-3、ELECTRA、ALBERT                 |
| ......  | ......                                 |

## 一、预训练 Pre-Train

### 1.1、问题引出

​	传统的机器学习算法侧重于运用数学、统计等方法对数据进行可解释的建模、特征抽取等（如《统计学习方法》），而深度学习则面向大量数据，利用模型去学习数据中的隐含特征；然而很多情况下，深度学习问题没有足够的、有效的数据支持其训练一个完整的性能可靠的模型，因此，预训练思想应运而生

### 1.2、思想来源

> 举个例子：
>
> 任务：有30张猫和狗的图片，要求训练一个模型 A 将猫狗分类

​	该任务的训练数据明显不足以训练一个有效的模型，如果直接使用这些数据从头开始训练模型，必然导致模型的欠拟合，那么该如何解决这个问题？

> （假设）虽然没有现成的分类猫狗的模型，但是有人已经用大量数据训练了一个用于分类老虎和狼的模型 B ；通过对模型中不同深度的卷积层做分析，人们发现，在模型的浅层，模型学习到的是一些通用的、抽象的特征（如：线条、纹理等）；那么，是否可以用这个模型已经学习的通用的抽象信息，来应用到新的模型之中呢？
>

### 1.3、预训练思想

​	事实证明，确实可以用 B 模型中已经学习的抽象信息（如模型的前 X 层），再辅以新任务的数据集，做到用小数据集实现较好的模型性能；具体方法分两种：

1. 冻结：浅层参数使用B模型的参数并保持不变，只利用新数据训练深层参数（根据具体任务添加的网络层）
2. 微调：浅层参数使用B模型的参数，然后利用新数据训练整个网络（原网络层与新添加的网络层）

​	需要注意的点：

1. 任务 A 和 B 的任务目标应尽量相似
2. 即使任务 A 的数据量支持它本身训练一个模型，但利用模型 B 进行预训练在大部分情况下仍然可以减少训练成本

​	**总结：预训练，即一个任务对应的模型 A 的参数不再是随机初始化的，而是通过已有任务进行预先训练得到模型 B，然后利用模型 B 的参数对模型 A 进行初始化，再通过任务 A 的数据对模型 A 进行训练的一种思想**

## 二、语言模型 LM

​	语言模型通俗点讲就是 **计算一个句子的概率** 

> 个人理解：判断一个句子是不是 `人话` ，计算出的概率大，就是人话，即，是一个正常的句子

​	对于语言序列 
$$
w_1,w_2,\cdots,w_n
$$
​	语言模型就是计算该序列的概率，即 
$$
P(w_1,w_2,\cdots,w_n)
$$
考虑两个具体实例：

1. 给定两句话 `判断这个词的磁性` 和 `判断这个词的词性` ，语言模型会认为后者更自然，转化成数学语言即：
   $$
   P(判断，这个，词，的，词性) \gt P(判断，这个，词，的，磁性)
   $$

2. 给定一句话 `判断这个词的___` 要求语言模型做填空，问题就变成了给定前面的词，找出后面的词是什么，转化成数学语言即：
   $$
   P(词性|判断，这个，词，的) \gt P(磁性|判断，这个，词，的)
   $$

通过上述两个实例，可以给出语言模型更加具体的描述：给定一句由 `n` 个词组成的句子 
$$
W=w_1,w_2,\cdots,w_n
$$
计算这个句子的概率
$$
P(w_1,w_2,\cdots,w_n)
$$
或者根据上文计算下一个词的概率 
$$
P(w_n|w_1,w_2,\cdots,w_{n-1})
$$
语言模型又分为 **统计语言模型** 与 **神经网络语言模型**

### 2.1、统计语言模型

统计语言模型的本质就是 **计算条件概率**

给定一句由 `n` 个词组成的句子 
$$
W=w_1,w_2,\cdots,w_n
$$
计算这个句子的概率 
$$
P(w_1,w_2,\cdots,w_n)
$$
的公式如下：（即：条件概率乘法公式的推广——链式法则）
$$
\begin{align*}
P(w_1,w_2,\cdots,w_n) 
& =  P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\cdots p(w_n|w_1,w_2,\cdots,w_{n-1}) \\
& = \prod_i P(w_i|w1,w_2,\cdots,w_{i-1})
\end{align*}
$$
对应到实例中（继续使用上一节的例子），即：
$$
\begin{align*}
& P(判断，这个，词，的，词性) = \\
& P(判断)P(这个|判断)P(词|判断，这个) \\
& P(的|判断，这个，词)P(词性|判断，这个，词，的)P(判断，这个，词，的，词性)
\end{align*}
$$
那么，对于上一节提到的另一个问题：当给定上文 `判断，这个，词，的` 时，下文（填空内容）的概率计算如下：
$$
P(w_{next}|判断，这个，词，的)\quad\text{(1)}
$$

> 其中，
> $$
> w_{next} \in V
> $$
>  表示词序列的下一个词，`V` 是一个具有 `|V|` 个词的词典（集合）

由条件概率乘法公式， `（1）` 式 可以展开为：
$$
P(w_{next}|判断，这个，词，的) = \frac{count(w_{next}，判断，这个，词，的)}{count(判断，这个，词，的)} \quad\text{(2)}
$$
那么对于`（2）`式，我们把字典 `V` 中的所有单词逐一作为 ***W**next* 代入计算，最后取使`（2）`式最大的词作为候选词

#### 2.1.1、n 元语言模型

​	根据以上推导可以发现，当语句中的词数量越来越多时，即 `|V|` 越来越大时，条件概率的计算会非常困难，那么不妨将单次计算中用到的 `|V|'` 缩小，只保留与预测值最相关的 `n` 个词来计算

> 即马尔科夫链思想：假设 ***W**next* 只和它之前的有限状态（有限长度的上文）有关

​	如：当 `n = 2` 时，为二元语言模型 ，`（2）`式可以简化为：
$$
P(w_i|w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1})}\quad\text{(3)}
$$
​	此时计算就会简单许多，但是同时抛弃了较远处的上文信息

​	`n` 的值可以根据任务需要进行调整，统称 `n元语言模型`

#### 2.1.2、平滑处理

​	根据 `（3）`式，注意到，由于文本数据的稀疏性，不可避免地出现想要计算的词的概率为0的情况，这就会造成条件概率计算为0，如：
$$
P(世界|你好)=\frac{count(你好,世界)}{count(你好)}
$$
​	在语料库为 `判断，这个，词，的，词性，磁性` 时，概率为 `0/0` ，这显然不利于模型训练，为了避免这种情况，会使用一种平滑策略：分子和分母都加上一个正数，如`（3）`式可以修正为：
$$
P(w_i|w_{i-1}) = \frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+|V|}
$$

### 2.2、神经网络语言模型

#### 2.2.1、起源

​	最开始，有人用双层感知机对文本进行预测，模型结构如图：

<img src=".\图片\2.2.1 神经网络语言模型.jpg" style="zoom: 80%;" />

​	其任务是，输入某个句中单词 ***W**t = X* 的前 `t-1` 个单词，要求网络正确预测单词 `X`，即最大化：
$$
P(w_t=X|w_1,w_2,\cdots,w_{t-1};\theta)
$$
​	该模型分为三层：

1. 输入层：将 `n-1` 个单词的 One-hot 编码作为输入，之后乘以一个随机初始化矩阵 `Q` （可学习的参数）后获得每个词对应的 `C` （词向量的由来），合并后得到该层的输出，记 `x` ：
   $$
   x=(C(w_1),C(w_2),\cdots,C(w_{t-1}))
   $$

2. 隐藏层：记权重矩阵为 `H` ，记偏置为 `d` ，激活函数为 `tanh` 

3. 输出层：一共有 `|V|` 个输出节点，记权重矩阵为 `W` ，记偏置为 `b` ，激活函数为 `Softmax` ，即每个输出节点表示词典中一个单词的概率值，公式化可记为：
   $$
   y = softmax(Wx+b+U\tanh(Hx+d))
   $$

#### 2.2.2、词向量的发现

​	通过以上模型，有人发现，除去模型输出结果不谈，反而是第一层的输出 `C` 具有许多优点：

1. 可以表示词
2. 可以将独热编码进行压缩（通过改变参数 `Q` 矩阵的维度）从而解决独热编码的稀疏问题，顺便节省了存储空间
3. 因独热编码相乘的矩阵 `Q` 通常非0，则 `C` 也通常非0，因此就可以利用这一特点描述两个词之间的相似度（余弦）,因此比较不同选词的两个句子的概率问题也就迎刃而解
4. 还可以根据不同任务对 `Q` 进行针对性改造（如Bert中的下游任务改造）从而适应不同的下游任务
5. ......

​	因此，神经网络语言模型的里程碑式`副产物`，**词向量**出现了！

## 三、词向量 Word2Vec

### 3.1、Word2Vec 介绍

​	Word2Vec 模型结构和 2.2.1 节 中的NNLM模型类似，如图：

<img src=".\图片\3.1 word2vec.jpg" style="zoom:80%;" />

​	Word2Vec 有两种训练方法：

1. CBOW：从一个句子里面把一个词抠掉，用这个词的**上文**和**下文**去预测被抠掉的这个词（BERT 的 MLM 方法思想来源）
2. Skip-gram：和 CBOW 正好反过来，输入某个单词，预测它的**上下文**

​	Word2Vec 和 NNLM 的架构类似，但是目的不同：

1. NNLM 通过对输入上文进行处理，并使用了一系列激活函数，为了使预测尽可能的准确
2. Word2Vec 则是不追求对预测结果的准确性，而是追求在预测过程中得到一个可以很好地表示词语之间的关系的矩阵 `Q` 

> 举个例子：
>
> CBOW 方法中，对于句子 ”你吃饭了吗？“，预测 ”你吃__了吗“ 中应填的词，在这个过程中，模型通过不断改变矩阵 Q 使其输出尽量接近 ”饭“ 这个词，但是是否必须为 ”饭“ 并不重要，即使输出为 ”肉“、”米饭“、”蔬菜“ 等，矩阵 Q 也已经具备了将相近语义的词压缩到相近空间中的能力，这个 Q 即为 Word2Vec 模型所追求的。
>
> Skip-gram 同理。

​	因此，Word2Vec 模型的计算也相对简单，比如隐藏层没有像NNLM中用tanh做激活等

### 3.2、缺陷

​	尽管 Word2Vec 可以得到词向量，但是这种方式得到的词向量在使用中难以解决多语义的问题

> 举个例子：
>
> 假如已经通过 Word2Vec 对语句 ”我喜欢吃苹果“ 进行训练得到可以表示 ”苹果“ 的词向量
>
> 现有一任务关于语句 ”我喜欢苹果手机“ ，则不同 ”苹果“ 的含义会造成词向量表现不佳

那么，如何解决这一问题？

## 四、基于语境的向量表示 ELMo

### 4.1、ELMo 介绍

​	目的：解决多义词问题

​	方法：在词向量中包含上下文信息，这样相同的单词在不同的上下文中对应不同的词向量

> ELMo 的思想概括：
>
> ​	事先用语言模型学好一个单词的 Word Embedding（同上文中Word2Vec生成的简单词向量，下同），在此阶段无法区分多义词
>
> ​	然后在实际使用 Word Embedding 的时候，单词必定具备特定的上下文，这时再根据上下文去调整单词的 Word Embedding 表示，特定的上下文会强化某种特定的语义而弱化其它语义，这样调整后的 Word Embedding 更能表达在这个具体上下文中的正确含义，也就解决了多义词问题
>
> ​	即， ELMo 本身是个根据当前下游任务的上下文对 Word Embedding 动态调整的模型

![](.\图片\4.1 ELMo.jpg)

ELMo生成的词向量包括：词语本身、词语被双向LSTM提取后的上下文特征（作者称为句法和语义特征）

ELMo 的实际运用包含两阶段过程：

1. 利用语言模型进行预训练
2. 在做下游任务时，从预训练网络中提取对应单词的网络各层的 Word Embedding 作为新特征补充到下游任务中

​	上图展示的是其第一阶段预训练过程，其网络结构采用了双层双向 LSTM，在此阶段语言模型训练的任务目标是根据单词 ***W**i* 的上下文去正确预测该词， ***W**i* 之前的单词序列 Context-before 称为上文，之后的单词序列 Context-after 称为下文

​	图中左端的前向双层 LSTM 代表正向编码器，输入的是从左到右顺序的除了预测单词 ***W**i* 外的上文 Context-before；右端的逆向双层 LSTM 代表反向编码器，输入的是从右到左的逆序的除了预测单词 ***W**i* 外的下文Context-after；每个编码器的深度都是两层 LSTM 的叠加

​	训练好这个网络后，输入一个新句子 ***S**new* ，句子中每个单词都能得到对应的三个 Embedding：

* 最底层：单词的 Word Embedding；
* 往上第一层：双向 LSTM 中对应单词位置的 Embedding，这层编码单词的句法信息更多一些；
* 往上第二层：双向 LSTM 中对应单词位置的 Embedding，这层编码单词的语义信息更多一些。

也就是说，ELMo 的预训练过程不仅仅学会单词的 Word Embedding，还学会了一个双层双向的 LSTM 网络结构（信息包含在内）

### 4.2、ELMo 应用

![](.\图片\4.2 ELMo 训练后的使用.jpg)

​	上图展示了 ELMo 如何用于下游任务（如 QA 问题）的使用过程，此时对于问句 `X`：

1. 先将句子 `X` 作为预训练的 ELMo 网络的输入，这样 `X` 中每个单词在 ELMo 网络中都能获得对应的三个 Embedding（上节提到）；
2. 然后给这三个 Embedding 各一个权重 ***W**i*，其可以通过学习过程获得，然后根据各自的权重累加求和，将三个 Embedding 压缩成一个特征；
3. 最后将压缩后的 Embedding 作为 `X` 在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用;

​	因为 ELMo 给下游任务提供的是每个单词的特征形式（压缩后的 Embedding），所以这类预训练方法被称为 `Feature-based Pre-Training` 即基于特征的预训练。

### 4.3、一些不足

1. 由于 ElMo 计算某个词时，总是要将其上下文包含在内进行计算，所以它的计算代价较高
2. 由于 ELMo 在使用上下文时采用简单拼接的方法，即对于一个中心词，其上文和下文是由两个部分在不同时刻生成的，所以在并行计算、特征提取等方面有待改进（如：Transformer 架构，以及运用了 Transformer 思想的 GPT 与 Bert）

那么，如何对这些缺陷进行改进，得到更优秀的词向量表示？

## 五、注意力机制 Attention

### 5.1、起源 - 人类的视觉注意力

​	视觉注意力机制，是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，即注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。

​	这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。

### 5.2、深度学习中的 Attention 思想

​	深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。尤其是在当今模型、参数、数据向大、多、广方向发展时更是需要引入注意力机制来筛选更关键的信息与特征。

​	但是首先要明确，运用注意力机制的前提是有使用注意力的主体，比如人去观察一幅图片，这幅图片上的信息的重要程度是观察者主观决定的，如果把观察者的一次观察记为一次 `Query` ，被观察的图片等信息记为 `Values` ，那么注意力就是要计算并找出与本次 `Query` 最相关的信息加以提炼，即为 Attention 的输出

​	总结即：注意力机制通过给定的 `Query` 与 `Values` ，抽取重要信息

### 5.3、键值对注意力

> 硬性注意力：感觉用的不多，略

![](.\图片\5.3 键值对注意力.jpg)

> ​	↑ ：邱锡鹏《神经网络与深度学习》附图 键值对注意力

![](.\图片\5.3 键值对注意力_计算图.png)

​	具体，如上图，Attention 计算过程可以表示为将 `Query(Q)` 和 `key-value`（把 Values 拆分成了键值对的形式，图中最上面和最下面的输入） 映射到输出上，其中 `query`、每个 `key`、每个 `value` 都是向量，输出是所有 `values` 的加权，其中权重是由 `Query` 和每个 `key` 计算出来的，记 `K、V、Q` 为以上三者的集合，具体计算分为三步：

​	**第一步：**计算 `Q` 和 `K` 的相似度，用 `F(Q,K)` 来表示：
$$
f(Q,K_i)\quad i=1,2,\cdots,m
$$
此步计算方法又包括以下几种

1. 点积（**Transformer 中使用**）：
   $$
   f(Q,K_i) = Q^T K_i
   $$

2. 缩放点积（**Transformer 中 Normalize 的目的**）：
   $$
   f(Q,K_i) = \frac{Q^T K_i}{\sqrt d_k}
   $$

   > 其中，***d**k* 为输入向量的维度

3. 权重：
   $$
   f(Q,K_i) = Q^TWK_i
   $$

4. 拼接权重：
   $$
   f(Q,K_i) = W[Q^T;K_i]
   $$
   
5. 感知器：
   $$
   f(Q,K_i)=V^T \tanh(WQ+UK_i)
   $$
   

​	**第二步：**将得到的相似度进行 softmax 与 归一化（缩放点积）：
$$
\alpha_i = softmax(\frac{f(Q,K_i)}{\sqrt d_k})
$$

> ​	缩放点积的作用：假设 Q 、K 里元素的均值为0，方差为 1，那么记： 
> $$
> A^T=Q^TK
> $$
> 中元素的均值为 0，方差为 d（维度）
>
> ​	当 d 很大时， A 中元素的方差也会变得很大，如果 A 中的元素方差很大（分布的方差大，分布集中在绝对值大的区域），此时 softmax 将几乎全部的概率都分配给了最大值对应的标签，进而会导致 softmax 函数出现梯度消失问题
>
> ​	总结一下就是
> $$
> \operatorname{softmax}\left(A\right)
> $$
> 的分布会和 d 有关，因此 A 中每一个元素乘上
> $$
> \frac{1}{\sqrt{d_k}}
> $$
> 后，方差压缩为 1，且 A 对应数值的数量级也会变小，有助于解决以上问题。

​	**第三步：**针对计算出来的权重 ***α**i*，对 V 中的所有 values 进行加权求和计算，得到 Attention 输出向量，记：
$$
Attention = \sum_{i=1}^m \alpha_i V_i
$$
至此即可实现：通过计算出的不同信息对于查询 `Q` 的重要程度，即从信息中挑出这些重要的信息的功能

### 5.4、自注意力机制 Self-Attention

​	广义的注意力机制中，我们并不知道 `Q、K、V` 如何获得或者来自哪里，是人工构造？还是通过别的方法学习？

​	而自注意力机制，如其名称所言，就是通过 `输入序列与输入序列自身做运算`（利用了语言序列中天然含有的依赖关系、指代关系、词法语法语义信息等），得到注意力机制中的 `Q、K、V` ，并据此抽取输入序列自身的重要信息的机制（个人tips：我和我自己作比较并发现自己的优点）

> 可以理解为，注意力机制是一个大的概念，它包含自注意力机制、后面的Masked自注意力机制、多头自注意力机制等具体算法

​	具体结构如下：

![](.\图片\5.4 Self-Attention.jpg)

​	从图中可以看到 Self Attention 有三个输入 `Q、K、V`，且它们都来自句子 `X` 的词向量 `x` 的线性转化，即对于词向量 `x`，给定三个可学习的参数矩阵：
$$
W_Q,W_k,W_v
$$
然后用 `x` 分别右乘上述矩阵即可得到 `Q、K、V`

#### 5.4.1、一个具体计算过程

​	对于一个具体例子，输入序列为 Thinking Machines，具体计算过程如下：

**第一步：**Q、K、V 的获取

![](.\图片\5.4.1 自注意力之 Q K V 的获取.jpg)

​	图中，两个单词 Thinking 和 Machines 通过线性变换，即 `xi` 和 `x2` 两个向量分别与`Wq、Wk、Wv` 三个矩阵点乘得到 `q1、q2` 、 `k1、k2` 、 `v1、v2` 共 6 个向量，分别拼接后得到矩阵 `Q、K、V`

**第二步**：矩阵乘，计算当前查询向量 `qi` 与整个序列的键向量 `ki` 的乘积（其实就是计算相关性）

![](.\图片\5.4.1 自注意力之 Q-K 乘积.jpg)

​	如图，向量 `q1、k1` 点乘得分 112， `q1、k2` 点乘得分96，即通过 `q1` 这个查询（也是信息）找到 `x1、x2` 中较重要的信息

**第三、四步**：归一化（Scale）与 Softmax

![](.\图片\5.4.1 自注意力之 QK-scale-softmax.jpg)

​	如图，对上一步中的得分进行规范化，除以
$$
\sqrt {d_k} = 8
$$
并进行softmax

> 注：因为输入序列 `X` 中最长的单词的维度为 8

**第五步**：矩阵乘，通过将上一步 Softmax 的输出（可以视为 得分比例、重要程度等，好理解）与 Values 相乘，计算最终选择的信息

![](.\图片\5.4.1 自注意力之 softmax-Values 乘积 .jpg)

​	如图，用得分向量 [0.88，0.12] 乘以 Values（本例为 `v1、v2` ）得到一组加权后的向量，将这些向量累加即可得到最终输出 `zi` 

​	上述过程即为 Self Attention 模型关于一个单词的大致计算过程，扩展为矩阵运算即：

​	输入是一个 `2x4` 的矩阵（句子中每个单词的词向量的拼接），每个权重矩阵维度为 `4x3` ，通过以上步骤求得 Q、K、V

![](.\图片\5.4.1 自注意力之 QKV-矩阵表示.jpg)

​	然后 Q 对 KT 做点乘，除以 `dk的平方根`，再通过 softmax 得到得分比例矩阵（其 ∑ 为1），最后与 V 做点乘得到输出 Z，那么这个 Z 就是一个考虑过当前中心词上下文的输出

![](.\图片\5.4.1 自注意力之 QKVZ矩阵 结果.jpg)

> ​	注意看这个公式，`Q x KT` 其实就是一个 word2word 的 attention map ，加了 softmax 之后就是一个合为 1 的权重
>
> ​	比如输入一句话 `i have a dream` 总共 4 个单词，这里就会形成一张 4x4 的注意力图：
>
> ![](.\图片\5.4.1 自注意力之 注意力矩阵.jpg)
>
> ​	其中两两单词之间都会有一个权重，**此即 Self Attention 名字的来源，即 Attention 的计算来源于 Source（源句） 和 Source 本身，通俗点讲就是 Q、K、V 都来源于输入 X 本身。**

#### 5.4.2、自注意力学到了什么

![](.\图片\5.4.2 自注意力之 学习句法特征.jpg)

​	自注意力机制可以捕获同一个句子中单词之间的一些句法特征，如上图所示，学习到的有一定距离的短语结构

![](.\图片\5.4.2 自注意力之 学习语义特征.jpg)

​	自注意力机制还可以捕获同一个句子中的更高维的语义特征，如上图所示，它捕捉到单词 `its` 的指代含义为 `Law` ，而这些都是跨越了一定的空间距离的信息

#### 5.4.3、自注意力机制较 RNN、LSTM 的优势

​	从上节可以看出，无论输入序列有多长，Self Attention 都可以充分捕获单词间的依赖关系，因为 Self Attention 在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤（矩阵乘）直接联系起来，所以远距离相互依赖的特征之间的距离被极大缩短（计算层面），有利于有效地利用与提取这些特征，因此可以很好的提取句法、语义特征；

​	另一方面，Self Attention 对于序列中每个单词都单独进行 Attention 值的计算，即 Self Attention 对计算的并行性也有很好的支持，即每个单词的计算都是可以并行处理的。

​	而 RNN 、 LSTM，则需要依次对序列进行计算，对于远距离的相互依赖的特征，必定要经过一定步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小，且过长的距离所产生的计算又会导致优化层面的梯度消失、梯度爆炸、收敛困难等问题，对序列的依赖性也导致它们无法有效对计算步骤进行并行处理优化。

​	但是！理论上 Self-Attention （Transformer 50 个左右的单词效果最好）解决了 RNN 模型的长序列依赖问题，但是由于文本长度增加时，训练时间也将会呈指数增长，因此在处理长文本任务时可能不一定比 LSTM（200 个左右的单词效果最好） 等传统的 RNN 模型的效果好，具体问题要具体分析！

### 5.5、掩码自注意力机制 Masked Self-Attention

​	在自注意力机制中，对 Attention 矩阵的计算，模型始终可以获知完整的输入与输出（它自身）序列，而在一些特定任务如：文本生成、机器翻译、机器问答等，模型不可能在一开始就得到未来的信息来用于当前的计算，所以掩码自注意力机制应运而生

> 一个例子（可能不够严谨）：
>
> 依旧使用上节的例子，在生成序列 `I have a dream` 的过程中，有以下几步：
>
> 1. 生成 `I` ，此时没有任何其他信息参与注意力计算，`I` 只能和它自身做注意力计算且结果为 1 （因为 softmax 输出和为 1 ）
> 2. 生成 `I have` ，此时有两个单词互相做注意力计算
> 3. 生成 `I have a` ，同理
> 4. 生成 `I have a dream` ，此时才得到完整的序列，通过自注意力计算发现句子已经输出完整（满足任务需求）
> 5. 生成 `I have a dream <eos>` ，结束输出

​	总之，掩码 mask 就是对自注意力机制生成的 Attention 矩阵沿着对角线把灰色的区域覆盖掉，不给模型看到未来的信息，模型只能通过当前已有的信息对未来信息做推断

![](.\图片\5.5 掩码自注意力之 掩码注意力矩阵.jpg)

> ↑：初始状态
>
> ↓：计算并 softmax 输出后的结果

![](.\图片\5.5 掩码自注意力之 掩码注意力矩阵softmax结果.jpg)

> 注：Transformer 模型里会再次探讨 Masked Self-Attention
>

### 5.6、多头注意力机制 Multi-head Attention

#### 5.6.1、问题引出

> ​	为什么使用多头注意力？

​	从机器学习的本质角度理解，词向量就是在高维空间中把原本不合理的词向量（如 One-hot 编码）所表示的点，通过一系列非线性变化（如 sigmoid 类函数），把它转移到合适的位置（包括空间的变换与伸缩），这个变换后的位置包含了它的词法信息、句法信息、语义信息等，越 “正确” 的位置，它对这个词的信息的提炼越完善

​	Word2Vec、ELMo、自注意力机制等都可以获得一个变换后的位置，但是位置的准确程度却不一，那么如何提高准确程度？

​	多头注意力机制运用了类似集成学习的思想：既然一次注意力计算可以得到一个相对准确的位置，那么使用多个注意力计算，得到多个候选结果，然后再用一定的方法，从中挑选或拟合一个最为准确的位置，从而可以提高词向量的表示能力

#### 5.6.2、具体结构与过程

![](.\图片\5.6.2 多头注意力.png)

​	如图，Multi-Head Attention 就是把 Self Attention 的 `X` 切分（通过与不同的变换矩阵相乘），然后各自通过注意力机制的计算成 `n 个 Zi`，然后通过全连接层获得新的 `Z'`，通常多头注意力中的 `n` 取 8 ，具体流程如下：

![](.\图片\5.6.2 多头注意力之 计算过程.jpg)

> 切分过程是通过初始输入 `X` 与不同的权重矩阵相乘实现的，其余步骤和注意力机制相同
>
> 合并过程首先将不同的 `Zi` 拼接，然后用矩阵变换的形式重新将结果压缩为原始维度

### 5.7、注意力机制总结

优点：

1. 解决了长距离依赖问题
2. 解决了并行计算问题

缺点：

1. 计算开销变大了
2. 由注意力机制的原理可以发现，计算注意力时会弱化甚至抹消句子中词语之间的顺序关系，即使打乱输入序列，最终计算出的注意力或词向量仍不会变，相较之下，传统 RNN 模型由于计算过程中天然蕴含了词语之间的先后关系，在捕捉这类位置信息上比注意力机制更优秀

那么，既然注意力机制中没有蕴含词语的位置信息，怎么办？自己加一个！

## 六、位置编码 Position Embedding

​	位置编码的使用很简单，在 Attention 中，为输入的 `X` 叠加一个 位置编码即可：

![](.\图片\6 位置编码.jpg)

​	如图，对于输入 `X` 进行 Attention 计算之前加上位置信息，即 `X` 的词向量为
$$
X_{final\_embedding} = Embedding + Positional\, Embedding
$$
但是，如何获得或者计算位置编码 `T` ？

​	不同模型中使用的位置编码计算方式不同，Transformer 中，使用 sin 与 cos 常量位置编码（我自己起的名字），具体如下：
$$
PE(pos,2i) = sin(pos/10000^{2i/d_{model}})\\
PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}})
$$

> 其中：
>
> pos 是当前词在序列中的位置，i 是词向量中的维度，**d**model 是词向量的维度
>
> 2i 和 2i+1 只是将序列平均分为两部分的标识，因为通过两部分交替使用 sin 和 cos ，就可以实现靠后位置的位置编码可以通过靠前位置的位置编码的线性组合来表示（有点类似动态规划的思想）↓↓↓

​	其思想来源于三角函数中的两角和公式：
$$
sin(α+β) = sinαcosβ+cosαsinβ\\
cos(α+β) = cosαcosβ-sinαsinβ
$$
如果把 α 和 β 看作序列中的位置，那么 α+β 位置的信息就可以用 α 的位置信息和 β 的位置信息线性组合而得到，从而可以理解为位置靠后的位置信息包含位置靠前的所有位置信息的一部分，这样就可以体现出位置上的前后关系（相对位置），从而可以理解为对序列的顺序进行编码

> 举个例子（可能不够严谨）：
>
> 比如输入序列为 `我、爱、吃、苹果` ，那么对于 `苹果` 的位置编码（记位置为4），由公式可以理解为 PE（4，2*2）= sin（4）= sin（α+β）= sin（1+3）= sin（2+2）= ...
>
> 其中 1，2，3 均可理解为 `苹果` 之前的词的 pos ，通过运算使它们被包含在了 `苹果` 的 pos 信息中

## 七、Transformer 架构

### 7.1、模型架构

​	Transformer 起初作为一个 seq2seq 的翻译模型而出现，这里用一个机器翻译的例子从整体到局部来分解一下 Transformer 框架：

![](.\图片\7.1 Transformer 顶层.jpg)

​	首先，Transformer 相当于一个黑箱，输入序列，会输出序列对应的翻译结果

![](.\图片\7.1 Transformer 一层 编解码器.jpg)

​	将黑箱拆分，可见 Transformer 整体分为编码器（encoder）和解码器（decoder）两部分，输入序列经过编码器编码后，再输入给解码器进行解码输出

![](.\图片\7.1 Transformer 二层 编解码器内部.jpg)

​	再次将编解码器拆分，可见 Transformer 的编解码器各自默认包含 6 个更细分的编解码器单元，输入序列通过编码器层层递进进行编码，最后一层的输出结果再分别交给不同层级的解码器进行处理

![](.\图片\7.1 Transformer 三层 编解码器单元.jpg)

​	再往下拆分，每个编码器单元由一个自注意力单元和一个前向单元（简单的前馈神经网络，主要作用是做 Relu 激活）组成，那么它的输出至此就很明确，即：输入序列对应的词向量矩阵

​	解码器单元也包括一个自注意力单元和前向单元，但解码器多了一个编解码器注意力单元

​	至此，可以将 Transformer 完整架构汇总为如下图所示：

<img src=".\图片\7.1 Transformer 完整框架.jpg" style="zoom:50%;" />

​	接下来，针对其中每个部分说明

### 7.2、编码器 Encoder

​	编码器模块与具体计算过程如图：

![](.\图片\7.2 Encoder 计算过程.png)

​	还是以 `Thinking Machines` 输入为例，从 底向上依次描述编码器计算过程：

1. 绿色的 `Xi` 代表输入序列初始编码，如通过 One-hot、Word2Vec等获得的简单词向量

2. 叠加位置编码，得到黄色的 `Xi`

3. 对带有位置编码的黄色的 `Xi` 序列进行 Self-Attention 计算，得出每个词的 Attention 词向量浅色 `Zi`

4. 将带有位置编码的原始向量经过残差网络与浅色 `Zi` 进行叠加（**防止梯度消失**），然后进行归一化（限制输出空间，**防止梯度爆炸**），得到维度与数值更加规范的词向量深色 `Zi`

5. 将深色 `Zi` 输入进前馈神经网络，其包含两层线性层与一层 Relu 激活层（**做非线性变换**）：
   $$
   FFN(X) = Relu((XW_1+b_1)W_2+b_2)
   $$

6. 将步骤 4 得到的深色 `Zi` 通过残差网络与前馈神经网络的输出进行叠加（**防止梯度消失**），然后进行归一化（限制输出空间，**防止梯度爆炸**）

7. 最终得到一个编码器子单元的输出向量 `R`

8. 通过 6 层以上计算（6 层编码器子单元）， Transformer 的编码器就通过 6 层非线性变换，构造出一个**包含了词法信息、句法信息、语义信息、位置信息、维度与数值适中的词向量**

> 注意： Transformer 中词向量一般为 512 维，即上述 `X、Z、R` 都具有相同维数

### 7.3、解码器 Decoder

​	解码器包括 3 个 sub-layers：

1. Masked multi-head self-attention，用于计算当前时刻已经生成的序列（输入到解码器的序列）的注意力

   > 为什么要做 Masked？在 “Transformer 动态流程展示 ” 小节最后有解释

2. Encoder-Decoder Attention，对 Encoder 的输入和 Decoder 的Masked multi-head self-attention 的输出进行 attention 计算；

   > 为什么要对 Encoder 和 Decoder 的输出一同做 attention 计算，二者的输出起到什么作用，这一问题在 “Transformer 动态流程展示” 小节有解释

3. 前馈神经网络层，与 Encoder 中的相同

### 7.4、Transformer 的输出处理

​	分析完 Transformer 编码和解码两大模块，Transformer 最终如何从解码器的输出词向量还原出需要得到的目标文本？

![](.\图片\7.4 Transformer 输出处理.jpg)

​	如图，Transformer 最后的输出处理其实就是将解码器的输出通过线性层 Linear 后接上一个 softmax

1. 线性层是一个简单的全连接神经网络，它将解码器产生的向量，记为 `A` ，投影（压扁）到一个更高维度的向量 `B` 上

   > 假设模型词汇表是10000个词，那么向量 B 就有10000维，每个维度分别对应每个词的得分

2. softmax 层将这些分数转换为概率，如图，选择概率最大的维度并生成对应的与之关联的单词作为此时刻的输出，最后将整个序列处理完就得到最终的输出啦！

![](.\图片\7.4 Transformer 最终输出.jpg)

### 7.5、Transformer 的整体动态处理过程

> 注：PDF 无法显示 gif 动画

​	初始时，解码器只有编码器的输入，解码器自身有一个启动参数，按照如图流程生成第一个单词：

![](.\图片\7.5 Transformer 动态过程1.gif)

​	生成第一个单词后，解码器就会把当前时刻已经输出的序列（用于产生查询 `Q`）作为自身的输入，和编码器生成的下一个词的词向量（用于产生 `K、V`）一起，做 Encoder-Decoder Attention 计算，最后再经过归一化等输出处理过程产生结果

> 个人理解：Encoder-Decoder Attention 即用已经生成的序列来生成查询向量，然后通过注意力机制，在编码器生成的候选向量中，挑选最可能的结果（把注意力放在最可能的结果上）
>
> 这一思想是 Attention 中的 Soft-Attention 机制

![](.\图片\7.5 Transformer 动态过程2.gif)

​	以图中的具体输入为例：

1. 输入 `je suis etudiant` 到 Encoders，然后得到一个 `Ke`、`Ve` 矩阵（Key_Encoder、Value_Encoder）

2. 输入 `I am a student` 到 Decoders ，通过 Masked Multi-head Attention 层（第一轮为起始信息，当后面的轮次 Decoder 有输出后会附加在一起）得到 `I am a student` 的 attention 查询向量 `Qd` （Query_Decoder），然后用 `Qd、Ke、De` 进行正常的 attention 计算，得到输出 `I` （第一轮）

   > 关于起始信息：
   >
   > ​	在 Transformer 模型进行机器翻译时，解码器(Decoder)最开始没有输出的时候，通常会使用一个特殊的开始符号（比如 `<start>` 或 `[BOS]` )作为输入
   >
   > ​	这个特殊符号被预先定义，并被添加到每一个要翻译的句子前面，标志着一个新的句子开始，用于帮助模型了解一个新的序列开始的位置，并为句子输出的生成提供上下文，它会在实际解码或生成翻译文本之前传递给解码器
   >
   > ​	当然，具体实现可能依赖于所用的 Transformer 模型的具体版本和配置

1. 重复步骤 2 ，直到序列处理完毕

经过对整个流程的梳理，现在就可以解决 7.3 节 的遗留问题：

#### 7.5.1、为什么 Decoder 中要用 Masked Attention

1. 在 Transformer 训练阶段：我们知道 `je suis etudiant` 的翻译结果就是 `I am a student`，所以我们把 `I am a student` 的 Embedding 输入到 Decoders 里面，在翻译第一个词 `I` 时：
   - 如果对 `I am a student`  attention 计算不做 mask，`am，a，student` 对 `I` 的翻译必定会有一定的贡献
   - 反之，翻译过程中，模型没有可以参考的信息，从头开始翻译

2. 在 Transformer 测试阶段：我们不知道 `我爱中国` 的翻译结果为 `I love China`，我们只能随机初始化一个 Embedding 输入到 Decoders 里面（起始信息），在翻译第一个词 `I` 时：
   * 无论是否做 mask，`love，China` 对 `I` 的翻译都不会产生贡献
   * 但是翻译了第一个词 `I` 后，随机初始化的 Embedding 有了 `I` 的 Embedding 信息作为补充，即翻译第二个词 `love` 时，`I` 的 Embedding 将有一定的贡献，但是 `China` 对 `love` 的翻译无贡献，随着翻译的进行，已经翻译的结果将会对下一个要翻译的词有一定的贡献和指导作用，**这一过程，和做了 mask 的训练阶段实现匹配，因此，Decoder 做 Mask，是为了让训练阶段和测试阶段行为一致，避免过拟合**

## 八、GPT 与 BERT

### 8.1、问题引出

​	由上文对 Transformer 模型的解析可知， Transformer 包含了两部分：Encoder 和 Decoder

1. ​	Encoder 可以实现对输入向量的高效的特征提取，使模型认知现实世界的事物，依据这个特点，BERT 被提出

   > 因为  Transformer 编码器可以提取不同模态的输入数据的特征，因此 Transformer 为多领域融合提供了新的研究方向

2. ​	Decoder 因其使用掩码注意力机制，使其可以很好的预测输入向量的后续输出，依据这个特点，GPT 被提出

![](.\图片\8.1 BERT-GPT-ELMo 类比.png)

​	将 ELMo、GPT、BERT 的模型结构进行对比，可以发现，后两者可以看作把 LSTM 替换为 Transformer 中的编解码器，从而实现更强大的特征提取能力以及预测能力，然后 BERT 提取得到特征向量，再把向量用于各种任务中；GPT 得到强大的预测能力

> BERT 侧重于认知、GPT 侧重于生成

​	GPT 由于使用了 Transformer 的 Decoder 端，其中包含了 Masked Multi-Head Attention，导致其只能从单向（上文）获取输入信息，因此 GPT 的架构受限于此，中间层构造为单向 Transformer-Decoder 网络，且 GPT 的输出就是具体任务的输出，需要用模型来预训练

​	而 BERT 使用了 Transformer 的 Encoder 端，它可以获取双向的输入信息，所以BERT 的中间层构造为双向 Transformer-Encoder 网络，更加类似于 ELMo 中的双向 LSTM 结构；BERT 得到的是从数据集中抽取的特征向量（认知了数据），而不是某个具体任务的结果，有了特征向量，再结合具体下游任务的改造，BERT 就可以实现强大的功能

​	BERT 的作者认为，使用自左向右编码和自右向左编码的单向编码器拼接而成的双向编码器，在性能、参数规模和效率等方面，都**不如直接使用深度双向编码器强大**，这也是为什么 BERT 使用 Transformer Encoder 作为特征提取器，而不使用自左向右编码和自右向左编码的两个 Transformer Decoder作为特征提取器的原因；但是作者也表示**双向编码器比单向编码器训练慢**，进而导致BERT 的训练效率低了很多，但是实验也证明 BERT 拥有超出同期所有预训练语言模型的语义理解能力，所以牺牲的训练效率是值得的

### 8.2、生成式预训练语言模型 GPT

> GPT：Generative Pre-Training

![](.\图片\8.2 GPT 两阶段.jpg)

​	GPT 的训练包含两阶段：

1. 利用 `语言模型` 进行预训练
   - GPT 的预训练类似于在预训练章节中的图像领域的预训练的例子，它要求预训练所使用的模型与最终任务需要的模型尽可能相似，比如想要训练一个 GPT 生成模型，那么预训练的语言模型就要是生成模型，而不应是问答模型或填空模型等
2. 通过 `Fine-tuning` 契合并完成下游任务
   - 首先，将目标任务的网络结构改造成和 GPT 一样的网络结构
   - 然后，用上一步预训练好的参数初始化 GPT 
   - 最后，用目标任务的训练数据继续训练这个 GPT ，并对网络参数进行微调（Fine-Tuning），最终得到一个契合具体任务的模型

![](.\图片\8.2 GPT Fine-Tuning.jpg)

> 在 BERT 中会详细说明下游任务改造, GPT 的下游改造思路是一样的

### 8.3、双向 Transformer 编码表示 BERT

> BERT：Bidirectional Encoder Representations from Transformers
>
> 重要意义：从 **大量无标记数据集** 训练得到的深度模型，通过学习数据中的隐含特征，可以显著提高各项 NLP 任务的准确率

#### 8.3.1、BERT —— 集大成者

1. 借鉴了了 ELMo 中的双向编码思想

   > 单项编码与双向编码：
   >
   > 例如填空预测：`今天天气很{___}，我们出去玩吧！`
   >
   > 单项编码可以利用的信息只有上文 `今天天气很` ,从而得知其预测结果包含许多形容词，如 `好、坏、不错、一般` 等
   >
   > 双向编码则可以利用完整上下文，在单向编码的基础上，由下文 `我们出去玩吧！` 可以排除候选词中的负面形容词，使得预测更精准

2. 借鉴了 GPT 中使用 Transformer 作为特征提取器的思想

   > GPT 使用 Transformer Decoder 作为特征提取器，具有良好的文本生成能力，然而当前词的语义只能由其前序词决定，并且在语义理解上有所不足
   >
   > BERT使用了 Transformer Encoder 作为特征提取器，并使用了与其配套的掩码训练方法，虽然双向编码让 BERT 不再具有强大的文本生成能力，但是 BERT 的语义信息提取能力十分强大

3. 借鉴了 Word2Vec 中的 CBOW 训练方法

#### 8.3.2、BERT 的模型结构

![](.\图片\8.3.1 BERT 模型结构.jpg)

​	BERT 的结构其实就是 Transformer Encoder（原文中称 Transformer Block）的堆叠

​	在模型参数选择上，论文给出了两套大小不一致的模型：

1. $$
   BERT_{BASE}
   $$

   hidden_size = 768（论文中的 H，下同）

   num_hidden_layers = 12（论文中的 L，下同）

   num_attention_heads = 12（论文中的 A，下同）

2. $$
   BERT_{LARGE}
   $$

   hidden_size = 1024

   num_hidden_layers = 24

   num_attention_heads = 16

   > 其中
   >
   > `hidden_size`：Transformer 隐藏层的大小
   >
   > `num_hidden_layers`：Transformer Block 堆叠层数
   >
   > `num_attention_heads`：Transformer 的 Multi-Head Attention 头数

##### 8.3.2.1、BERT-base 参数计算

在BERT-base中，主要参数有：

1. `vocab_size`： 30,000：输入词汇表大小
2. `hidden_size`：768
3. `num_hidden_layers`：12
4. `num_attention_heads`：12
5. `max_position_embeddings`：512：位置嵌入的最大维度
6. `intermediate_size`：3072：前馈神经网络隐藏层的大小，为 `hidden_size` * 4。

下面具体计算一下参数：

1. 词嵌入和位置嵌入参数：这两部分的参数都使用`embedding_size`个参数表示一个单词或者一个位置。所以两者加起来的总参数就是`2 * vocab_size * hidden_size + 2 * max_position_embeddings * hidden_size = 2 * 30,000 * 768 + 2 * 512 * 768 = 46,137,344`

2. Transformer 编码器参数：每层共有12个则自注意力头，每个头有四个权重矩阵（Q、K、V以及O），每个矩阵的大小都是`hidden_size * hidden_size / num_attention_heads`，所以一层encoder的自注意力机制有的参数数量是：`num_attention_heads * 4 * hidden_size * hidden_size / num_attention_heads = 4 * 768 * 768 = 2,359,296`。另外每层encoder还包含一个前馈神经网络（Feed Forward Neural Network, FFNN），其参数数量为`2 * hidden_size * intermediate_size = 2 * 768 * 3072 = 4,718,592`. 所以每层encoder的总参数数是`2,359,296 + 4,718,592 = 7,077,888`。故12层encoder的总参数数为`7,077,888 * 12 = 84,934,656` 

然后两部分相加得到BERT-base的总参数数是`46,137,344 + 84,934,656 = 131,072,000`，约为131M。

以上计算简化了一些细节，例如忽略了LayerNorm层和bias等极少量的参数，但这不影响计算的总体准确性。

#### 8.3.3、BERT 的无监督训练

![](.\图片\8.3.3 BERT 训练两阶段.png)

​	同 GPT ，BERT 也采用二段式训练方法：

1. 使用易获取的大规模无标签语料，来训练基础语言模型；
2. 根据指定任务的少量带标签训练数据进行微调训练。

​	不同于 GPT 等标准语言模型使用 
$$
P(w_i|w_1,\cdots,w_{i-1})
$$
为目标函数进行训练，能看到全局信息的 BERT 使用 
$$
P(w_i|w_1,\cdots,w_{i-1},w_{i+1},\cdots,w_n)
$$
为目标函数进行训练，并且 BERT有两种主要训练方法，分别用于不同任务：

1. 用语言掩码模型（MLM）方法训练词的语义理解能力
2. 用下句预测（NSP）方法训练句子之间的理解能力

##### 8.3.3.1、BERT 之语言掩码模型（MLM）

> MLM：Masked Language Model

​	由于无法使用标准语言模型的训练模式（因为双向编码器对所有信息的捕获能力），BERT 借鉴完形填空任务和 Word2Vec 中 CBOW 的思想，使用语言掩码模型（MLM ）方法训练模型

​	MLM 方法也就是随机去掉句子中的部分 token（单词），然后模型来预测被去掉的 token 是什么。这样实际上已经不是传统的神经网络语言模型(如生成模型)了，而是单纯作为分类问题，根据这个时刻的 hidden state 来预测这个时刻的 token 应该是什么，而不是预测下一个时刻的词的概率分布

​	随机去掉的 token 被称作掩码词，在训练中，掩码词将以 15% 的概率被替换成 `[MASK]`，也就是说随机 mask 掉语料中 15% 的 token，这个操作称为掩码操作

> 在CBOW 中，每个词都会被预测一遍，相当于每个词都经历了 mask 操作

​	但是这样简单地去设计 MLM 训练方法有一个弊端：在模型微调（训练）阶段或模型推理（测试）阶段，输入的文本中都是没有 `[MASK]` 的，进而产生由训练数据与预测数据不一致导致的性能损失

​	考虑到上述的弊端，BERT 并没有简单地用 `[MASK]` 替换掩码词，而是按照一定比例选取替换词：在选择的 15% 掩码词中，又按一定比例分为以下部分进行不同处理：

​	其中 80% 用 `[MASK]` 替换， 10% 用`任意的词`代替（该做法是为了让 BERT 学会根据上下文信息自动纠错），剩余 10% 不发生变化（该做法是为了缓解训练文本和预测文本的偏差带来的性能损失），举例如下：

<img src=".\图片\8.3.4 BERT MLM1.jpg" style="zoom: 33%;" />

<img src=".\图片\8.3.4 BERT MLM2.jpg" style="zoom: 33%;" />

<img src=".\图片\8.3.4 BERT MLM3.jpg" style="zoom:33%;" />

> 作者在论文中解释这样做的好处是编码器不知道哪些词需要预测的，哪些词是错误的，因此**被迫学习每一个 token 的表示向量**
>

##### 8.3.3.2、BERT 之下句预测（NSP）

> NSP：Next Sentence Prediction

​	很多 NLP 的下游任务，如问答和自然语言推断，都基于两个句子做逻辑推理，而语言模型并不具备直接捕获**句子间的语义联系**的能力，或者说**单词粒度的训练完成不了句子关系级的任务**，所以为了学会捕获句子之间的语义联系，BERT 采用了下句预测（NSP ）作为无监督预训练的一部分

​	NSP 具体做法：输入 BERT 的语句由两个句子构成，其中 50% 的概率将语义连贯的两个连续句子作为训练文本**（连续句对一般选自篇章级别的语料，以此确保前后语句的语义强相关）**，另外 50% 的概率将完全随机抽取两个句子作为训练文本（不相关），如：

- 连续句对：`[CLS]`今天天气很糟糕`[SEP]`下午的体育课取消了`[SEP]`

- 随机句对：`[CLS]`今天天气很糟糕`[SEP]`鱼快被烤焦啦`[SEP]`

> 其中：
>
> `[SEP]`  标签表示分隔符
>
>  `[CLS]` 表示标签用于类别预测，结果为 1时表示输入为连续句对；结果为 0是表示输入为随机句对

​	但是为什么在两句之前添加 `[CLS]` 后就可以捕获句间含义呢？首先我们知道，在 Self-Attention 中，输入序列的每个 token 都会与整个序列进行 Attention 计算，那么 `[CLS]` 字符也不例外，并且在 BERT 的高达12层的 Self-Attention 计算后，它本身已经蕴含了整个序列（两句话）的信息，最后再经过一定的输出处理（如 Softmax）就可以标识两句话之间的关系

> `[SEP]` 可以做到和 `[CLS]` 相同功能吗？答案是否定的，因为 `[CLS]` 具有**位置固定**和**唯一性**两个特点

​	最后，通过训练 `[CLS]` 编码后的输出标签，**BERT 可以学会捕捉两个输入句对的文本语义**，在连续句对的预测任务中，BERT 的正确率可以达到 97%-98%

#### 8.3.4、BERT 的下游任务改造

##### 8.3.4.1、句对分类

​	任务：给定两个句子，判断它们的关系。如判断二者是否相似（逻辑并列）、后者是否为前者的答案（逻辑递进）等

​	针对句对二分类任务，显然 BERT 的 NSP 思想已经将其解决了，不需要对输入数据和输出数据的结构做任何改动，直接使用与 NSP 训练方法一样的输入和输出结构就行。如图所示，句对用 `[SEP]` 分隔符拼接成文本序列，在句首加入判别标签 `[CLS]`，将其输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练

<img src=".\图片\8.3.4.1 句对分类.jpg"  />

​	针对句对多分类任务，则需要在句首判别标签 `[CLS]` 的输出特征向量后接一个全连接层和 Softmax 层，保证输出维数与类别数目一致，最后通过 argmax 操作（取最大值时对应的索引序号）得到相对应的类别结果

> 举个例子：
>
> 任务：判断句子 `“我喜欢你”` 和句子 `“俺中意你”` 是否相似
>
> 输入：`“[CLS]我喜欢你[SEP]俺中意你”`
>
> ...计算过程...
>
> 输出：`[CLS]` 标签对应输出为 [0.02, 0.98] ，分别对应不相似与相似的类别概率
>
> 结果处理：通过 argmax 操作得到分类类别为 1，即两个句子相似

##### 8.3.4.2、单句分类

​	任务：给定一个句子，判断该句子的类别。如判断情感类别、句子是否语义连贯等

​	针对单句二分类任务，也无须对 BERT 的输入数据和输出数据的结构做任何改动。如图所示，单句分类在句首加入判别标签 `[CLS]`，将句首标签所对应的输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练

<img src=".\图片\8.3.4.2 单句分类.jpg"  />

​	针对单句多分类任务，和句对多分类一样，也是需要在句首标签 `[CLS]` 的输出特征向量后接一个全连接层和 Softmax 层，保证输出维数与类别数目一致，最后通过 argmax 操作得到相对应的类别结果

> 举个例子：
>
> 任务：判断句子`“晴天米饭地球鼠标我”` 是否语意连贯
>
> 输入：`“[CLS]晴天米饭地球鼠标我”`
>
> ...计算过程...
>
> 输出：`[CLS]` 标签对应输出为 [0.99, 0.01]，分别对应不连贯与连贯的类别概率
>
> 结果处理：通过 argmax 操作得到分类类别为 0，即这个句子不是一个语义连贯的句子

##### 8.3.4.3、文本问答

​	任务：给定一个问句和一个蕴含答案的句子，找出答案在后句中的位置。例如给定问题（句子 A），在给定的段落（句子 B）中标注答案的起始位置和终止位置

​	文本问答任务的处理方式和前述任务就有了较大差别，无论是在优化目标上，还是在输入数据和输出数据的形式上，都需要做一些特殊处理：为了标注答案的起始位置和终止位置，BERT 引入两个辅助向量 s（start，答案子句的起始位置） 和 e（end，答案子句的终止位置）；如图所示，BERT 将句子 B 中的每一个词得到的最终特征向量 `Ti'` 经过全连接层后（把 BERT 提取后的深层的特征向量转化为用于标记答案位置的一维特征向量），分别与向量 s 和 e 求内积，对所有内积分别进行 softmax 操作，即可得到子句 
$$
Token_m（m\in [1,M]）
$$
作为答案的起始位置和终止位置的概率，最后，取概率最大的片段作为最终的答案

<img src=".\图片\8.3.4.3 文本问答.jpg"  />

​	文本回答任务的微调使用了两个技巧：

1. 用全连接层把 BERT 提取后的深层特征向量转化为用于判断答案位置的特征向量
2. 引入辅助向量 s 和 e 作为答案起始位置和终止位置的基准向量，明确优化目标的方向和度量方法

> 举个例子：
>
> 任务：给定问句 `“今天的最高温度是多少”`，再答案文本 `“天气预报显示今天最高温度 37 摄氏度”` 中标注答案的起始位置和终止位置
>
> 输入：`“[CLS]今天的最高温度是多少[SEP]天气预报显示今天最高温度 37 摄氏度”`
>
> ...计算过程...
>
> 输出：全连接层 Softmax 后的结果：
>
> |   答案文本   | 天气 | 预报 | 显示 | 今天 | 最高温 |  37  | 摄氏度 |
> | :----------: | :--: | :--: | :--: | :--: | :----: | :--: | :----: |
> | 起始位置概率 | 0.01 | 0.01 | 0.01 | 0.04 |  0.10  | 0.80 |  0.03  |
> | 终止位置概率 | 0.01 | 0.01 | 0.01 | 0.03 |  0.04  | 0.10 |  0.80  |
>
> 结果处理：对 Softmax 的结果取 argmax，得到答案的起始位置为 6，终止位置为 7，即答案为 `“37 摄氏度”`

##### 8.3.4.4、单句标注

​	任务：给定一个句子，标注其中每个词的标签。例如给定一个句子，标注句子中的人名、地名、机构名等

​	单句标注任务和 BERT 预训练任务具有较大差异，但与文本问答任务较为相似。如图所示，在进行单句标注任务时，需要在每个词的语义特征向量之后添加全连接层，将语义特征转化为序列标注任务所需的特征，单句标注任务需要对每个词都做标注，因此不需要引入辅助向量，直接对经过全连接层后的结果做 Softmax 操作，即可得到各类标签的概率分布

<img src=".\图片\8.3.4.4 单句标注.jpg"  />

​	由于 BERT 需要对输入文本进行分词操作，独立词将会被分成若干子词，因此 BERT 预测的结果将会是 5 类（细分为 13 小类）：

- ​	O（非人名地名机构名，O 表示 Other）
- ​	B-PER/LOC/ORG（人名/地名/机构名初始单词，B 表示 Begin）
- ​	I-PER/LOC/ORG（人名/地名/机构名中间单词，I 表示 Intermediate）
- ​	E-PER/LOC/ORG（人名/地名/机构名终止单词，E 表示 End）
- ​	S-PER/LOC/ORG（人名/地名/机构名独立单词，S 表示 Single）

​	将 5 大类的首字母结合，可得 IOBES，这便是序列标注最常用的标注方法

> 举个例子（命名实体识别 NER ）：
>
> 任务：给定句子 `“爱因斯坦在柏林发表演讲”` ，根据 IOBES 标注 NER 结果
>
> 输入：`“[CLS] 爱 因 斯坦 在 柏林 发表 演讲”`
>
> ...计算过程...
>
> 输出：全连接层 Softmax 后的结果：
>
> | BOBES |  爱  |  因  | 斯坦 |  在  | 柏林 | 发表 | 演讲 |
> | :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
> |   O   | 0.01 | 0.01 | 0.01 | 0.90 | 0.01 | 0.90 | 0.90 |
> | B-PER | 0.90 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
> | I-PER | 0.01 | 0.90 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
> | E-PER | 0.01 | 0.01 | 0.90 | 0.01 | 0.01 | 0.01 | 0.01 |
> | S-LOC | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
>
> 结果处理：对 Softmax 的结果取 argmax，得到最终 NER 标注结果为：`“爱因斯坦”` 是人名，`“柏林”` 是地名

#### 8.3.5、BERT 的成果展示

​	从上述章节可以看出，NLP 四大类任务都可以比较方便地改造成 Bert 能够接受的形式，只是不同类型的任务需要对模型做不同的修改罢了，但是修改都是非常简单的，比如加一层全连接、Softmax之类即可，这也是 Bert 的一大优点，意味着它几乎可以契合任何 NLP 的下游任务，具备相当程度的普适性，非常强！

​	因此，BERT 在 11 个各种类型的 NLP 任务中都达到了目前（论文时间）最好的效果，某些任务性能甚至有了极大的提升，具体如图：

> 图1：GLUE Test result

![](.\图片\8.3.5 GLUE.png)

> 图2、3：SQuAD 1.1 & 2.0 result

<img src=".\图片\8.3.5 SQuAD 1.1.png" style="zoom:50%;" />

<img src=".\图片\8.3.5 SQuAD 2.0.png" style="zoom:50%;" />

> 图4：SWAG Dev and Test accuracies

<img src=".\图片\8.3.5 SWAG Dev and Test.png" style="zoom:50%;" />

## 九、总结

​	我觉得 Bert 是 NLP 领域里程碑式的工作，对于后续 NLP 的研究和应用会产生长久的影响。但从以上的分析也可以看出，从模型或者方法角度，Bert 借鉴了 ELMo、GPT、CBOW等的思想，即 Bert 本身没有太大的创新，更像 NLP 领域发展路线上重要进展的集大成者：

1. 第一点，训练方面的两阶段模型
   1. 第一阶段为双向（注意不是单向）语言模型预训练
   2. 第二阶段采用具体任务 Fine-tuning 或者做特征集成

 	2. 第二点，特征抽取使用 Transformer 作为特征提取器而不是 RNN 或者 CNN
 	3. 第三点，双向语言模型想到了采取 CBOW 的方法去做

​	Bert 最大的亮点在于效果好及普适性强，几乎所有 NLP 任务都可以套用 Bert 这种两阶段解决思路，而且效果应该会有明显提升，可以预见的是，未来一段时间在 NLP 应用领域，Transformer 将占据主导地位，而且这种两阶段预训练方法也会主导各种应用

​	另外，预训练这个过程本质上是在做什么事情？其实就是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当现有任务的标注信息数据有限时，这些先验的语言学特征自然会对现有任务起到极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不完全的，因此模型泛化能力很弱，所以通过集成尽量通用的语言学知识自然会加强模型的泛化能力

​	如何引入先验的语言学知识其实一直是 NLP 尤其是深度学习场景下的 NLP 的主要目标之一，不过一直没有太好的解决办法，而 ELMO、GPT、Bert 的这种两阶段模式无疑是解决这个问题自然又高效的方法（目前为止），这也是这些方法的价值所在

## 十、参考资料

- 参考书籍
  - 《神经网络与深度学习》邱锡鹏
- 参考文献
  - Word2Vec：[Efficient Estimation of Word Representations in Vector Space (arxiv.org)](https://arxiv.org/abs/1301.3781)
  - ELMo：[Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
  - Transformer：[Attention Is All You Need (arxiv.org)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
  - GPT：[GPT-1 (mikecaptain.com)](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)
  - BERT：[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arxiv.org)](https://arxiv.org/abs/1810.04805)
- 参考网络资料
  - <a href='https://zhuanlan.zhihu.com/p/49271699' target='_blank'>从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a>
  - <a href='https://www.cnblogs.com/nickchen121/p/15071844.html' target='_blank'>Attention和Transformer详解</a>
  - <a href='https://zhuanlan.zhihu.com/p/37601161' target='_blank'>深度学习中的注意力模型（2017版）</a>
  - <a href='https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w' target='_blank'>细讲 | Attention Is All You Need</a>
  - <a href='https://zhuanlan.zhihu.com/p/44121378' target='_blank'>【NLP】Transformer模型原理详解</a>
  - <a href='http://jalammar.github.io/illustrated-transformer/' target='_blank'>The Illustrated Transformer</a>
  - [B站 水论文的程序猿-预训练语言模型的前世今生（合集）](https://space.bilibili.com/383551518/channel/collectiondetail?sid=463688&ctype=0)
  - [B站 五连单排一班-文本分析（合集）](https://space.bilibili.com/81480422/channel/collectiondetail?sid=74798)





> Author：侯世龙
